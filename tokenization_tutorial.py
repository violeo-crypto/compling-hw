# -*- coding: utf-8 -*-
"""tokenization_tutorial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ocC4TcvkCLVhhesfBn96NmPSzxhxTFmE

# Токенизация в NLP

## Введение

**Токенизация** – это процесс разбиения текста на минимальные единицы (токены): слова, подслова или символы.  
Она является первым шагом во многих NLP задачах: анализ тональности, машинный перевод, чат-боты, обучение языковых моделей.

В этом туториале мы разберём разные виды токенизации и библиотеки, которые их реализуют.

## 1. Простые методы токенизации

### 1.1. Разделение по пробелам
"""

text = "Hello, world! This is a test."
text.split()

"""
### 1.2. Регулярные выражения
"""

import re

text = "Hello, world! This is a test."
tokens = re.findall(r"\w+", text)
tokens

"""**Ограничения простых методов**:  
- Не учитывают пунктуацию.  
- Не работают с разными языками и морфологией.  
- Не подходят для серьёзных задач.

## 2. Токенизация с помощью библиотек

### 2.1. NLTK  
**Применение**: образовательные проекты, простая предобработка текста.  
**Ограничения**: работает медленно на больших корпусах.  
**Когда использовать**: для курсовых и лабораторных работ по основам NLP.
"""

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

from nltk.tokenize import word_tokenize, sent_tokenize

text = "Hello, world! This is a test."
print(word_tokenize(text))
print(sent_tokenize(text))

"""
### 2.2. spaCy  
**Применение**: промышленные проекты, морфология, синтаксис.  
**Ограничения**: большие модели, требует ресурсов.  
**Когда использовать**: для курсовых по синтаксису и морфологии, проектов по обработке естественного языка.
"""

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Hello, world! This is a test.")
[t.text for t in doc]

"""
### 2.3. Stanza  
**Применение**: многоязычные корпуса, морфология, синтаксис.  
**Ограничения**: медленнее spaCy, требует загрузки моделей.  
**Когда использовать**: в исследованиях и при работе с редкими языками.
"""

!pip install stanza
import stanza

stanza.download("en")
nlp = stanza.Pipeline("en")
doc = nlp("Hello, world! This is a test.")
[[word.text for word in sent.words] for sent in doc.sentences]

"""
### 2.4. MosesTokenizer (sacremoses)  
**Применение**: машинный перевод, предобработка текстов.  
**Ограничения**: устаревающий стандарт.  
**Когда использовать**: для экспериментов с классическими MT-системами.
"""

!pip install sacremoses

from sacremoses import MosesTokenizer

mt = MosesTokenizer()
mt.tokenize("Hello, world! This is a test.")

"""
### 2.5. gensim  
**Применение**: подготовка данных для моделей `word2vec`, `fastText`.  
**Ограничения**: базовая токенизация.  
**Когда использовать**: при обучении собственных векторных моделей.  
"""

!pip install gensim
from gensim.utils import simple_preprocess

simple_preprocess("Hello, world! This is a test.")

"""
### 2.6. HuggingFace Tokenizers  
**Применение**: трансформеры, нейросетевые модели.  
**Ограничения**: сложность, нужен GPU для обучения токенизаторов.  
**Когда использовать**: в курсовых и проектах с BERT, GPT, T5.
"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tokens = tokenizer("Hello, world! This is a test.")
tokens

"""
## 3. Посимвольная токенизация  
"""

list("Hello, world!")

"""## 4. Комбинированные методы  

- **Послоговая токенизация** (актуально для языков с чёткой слоговой структурой, например, японский).  
- **Гибридные методы**: совмещение слов и символов.  

Пример: статья *Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models*.

## 5. Сравнение токенизаторов  

Попробуем применить разные методы к одному и тому же тексту.

## 6. Практика

Возьмём новостной текст, токенизируем разными способами и сравним.
"""

article = 'Natural language processing (NLP) is a field of artificial intelligence\
that gives computers the ability to understand text and spoken words in much the same way human beings can.'

"""
NLTK
"""

import nltk #импорт библиотеки ntlk
nltk.download('punkt') #cкачиваем необходимые данные для токенизации
nltk.download('punkt_tab')

from nltk.tokenize import word_tokenize #импорт функции word_tokenize из модуля tokenize

nltk_tokens = word_tokenize(article) #токенизация NLTK к тексту
print("NLTK Tokens:", nltk_tokens) # вывод результата токенизации NLTK
print("NLTK Count:", len(nltk_tokens)) #вывод количества токенов, полученных с помощью NLTK
print() #пустая строка для разделения вывода

"""
SpaCy
"""

import spacy #импорт библиотеки spacy

nlp = spacy.load("en_core_web_sm") #загрузка предварительно обученной модели для английского языка
doc = nlp(article) #обработка текста с помощью spaCy pipeline
spacy_tokens = [token.text for token in doc] #извлечение текста каждого токена из обработанного документа

print("SpaCy Tokens:", spacy_tokens) #результат токенизации spaCy
print("SpaCy Count:", len(spacy_tokens)) #количество токенов, полученных с помощью spaCy
print() #пустая строка для разделения вывода

"""
Gensim
"""

from gensim.utils import simple_preprocess #импорт функции simple_preprocess из библиотеки gensim

gensim_tokens = simple_preprocess(article) #упрощенная предобработка gensim к тексту
print("Gensim Tokens:", gensim_tokens) #результат токенизации Gensim
print("Gensim Count:", len(gensim_tokens)) #количество токенов, полученных с помощью Gensim

"""## 7. Задача для самостоятельного разбора  

Прочитать статью:  
**"Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"** (https://arxiv.org/abs/1604.00788)

### Напишите ответы на вопросы:
1. Что значит Out-of-Vocabulary?  
2. Как эту проблемы решили авторы статьи "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"?
3. Какие еще типы токенизации мы разбирали на занятии?

Ваш ответ здесь:

**Out-of-Vocabulary (OOV)** -- это термин, который описывает слова или символы, отсутствующие в предопределенном словаре (вокабуляре) модели машинного перевода или другого NLP-приложения.

В контексте статьи проблема OOV возникает из-за того, что почти все модели нейронного машинного перевода (NMT) того времени использовали ограниченный словарь, обычно включающий 30 000–50 000 наиболее частотных слов. Любое слово, не вошедшее в этот список (например, редкие термины, имена собственные, слова из других языков или опечатки), становилось «неизвестным» (`<unk>` токен). Это приводило к потере информации и ухудшению качества перевода.

Авторы предложили гибридную слово-символьную модель, которая объединяет эффективность работы на уровне слов с гибкостью обработки символов. Согласно статье, модель основную часть перевода выполняет на уровне слов для скорости и эффективности, но обращается к символьному компоненту для обработки редких или неизвестных слов.

Архитектура решения:

Для исходного языка (например, английский): Символьная рекуррентная нейронная сеть (CharRNN) строит векторные представления (эмбеддинги) для всех слов, включая OOV. Это позволяет модели корректно обрабатывать любое входное слово, даже если его нет в основном словаре.

Для целевого языка (например, чешский): В процессе генерации текста, если следующее слово является редким или неизвестным, управление передается символьному декодеру. Этот декодер генерирует слово посимвольно, что позволяет создавать корректные словоформы, даже если само слово ранее не встречалось.

Ключевое преимущество заключается в эффективности, так как обучение и работа на уровне слов быстрее, чем чисто символьные модели.

Помимо концептуального разделения на словесную и символьную токенизацию, на занятии мы подробно разобрали конкретные инструменты и методы, которые выполняют эту задачу на практике. Их можно условно разделить на несколько групп:

1. На основе правил и регулярных выражений
Токенизация с помощью регулярных выражений (re): Самый базовый, но гибкий метод. Позволяет задавать собственные шаблоны для разделения текста. Например, re.findall(r'\b\w+\b|[^\w\s]', text) находит слова и знаки препинания.

MosesTokenizer (sacremoses): Стандартный инструмент в машинном переводе для предобработки текста. Выполняет сложную, но предсказуемую токенизацию по правилам (например, отделяет пунктуацию от слов, разворачивает сокращения).

2. Готовые библиотеки для NLP
NLTK (word_tokenize): Классическая библиотека. Предоставляет мощный токенизатор Punkt, который умеет корректно обрабатывать сокращения (напр., "Dr.", "U.S.A.") и контракции ("don't").

spaCy: Промышленная библиотека. Ее конвейер обработки начинается именно с токенизатора, который не просто делит текст, а создает объекты Doc и Token, сразу предоставляющие лингвистическую информацию (часть речи, лемма и др.).

stanza (StanfordNLP): Библиотека от Стэнфордского университета. Подобно spaCy, предоставляет комплексный конвейер, начинающийся с токенизатора, хорошо работающего для множества языков.

3. Фреймворки для работы с векторными представлениями и моделями
gensim (simple_preprocess): Библиотека для тематического моделирования и word2vec. Содержит упрощенные функции предобработки, которые часто включают в себя токенизацию по словам в нижнем регистре.

Hugging Face Tokenizers: Современная и высокопроизводительная библиотека, созданная для обучения и использования моделей, таких как BERT или GPT. Реализует субсловные алгоритмы (BPE, WordPiece, SentencePiece), которые и решают проблему OOV, разбивая редкие слова на значащие части.

4. Фундаментальный подход
Посимвольная токенизация: Разделение текста на отдельные символы (буквы, цифры, знаки). Это крайний случай, который полностью решает проблему OOV (любое слово можно собрать), но создает очень длинные последовательности и усложняет обучению моделей связи между символами.
"""